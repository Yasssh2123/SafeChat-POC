# AI Safety Pipeline - Proof of Concept

A comprehensive AI Safety system that integrates multiple ML models to enhance user safety in conversational platforms.

## ğŸ¯ Overview

This POC implements a multi-stage safety pipeline that processes messages through:

1. **Abuse Detection** - Identifies hate speech and offensive language
2. **Escalation Pattern Recognition** - Tracks conversation escalation over time  
3. **Crisis Intervention** - Detects suicide/self-harm indicators
4. **Age-Appropriate Filtering** - Ensures content suitability by age

## ğŸ—ï¸ Architecture

```
Message â†’ Abuse Check â†’ Escalation Check â†’ Crisis Check â†’ Age Filter â†’ Response/Alert
```

### Models Used

- **Offensive Language Detection**: DistilBERT classifier
  - Classes: `hate_speech`, `offensive_language`, `neither`
  - Path: `Offensive_Detection/Output/checkpoint-840`

- **Suicide Detection**: DistilBERT classifier  
  - Classes: `non-suicide`, `suicide`
  - Path: `Suicide_Detection/Output/checkpoint-1965`

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Download Pre-trained Models

Download and extract the trained models and datasets:

**Offensive Detection Model & Dataset:**
- Download: [Offensive_Detection.zip](https://drive.google.com/file/d/18o4EfAdLHJcpOyujx6LGGY4bwyJGmihz/view?usp=sharing)
- Extract to project root as `Offensive_Detection/`

**Suicide Detection Model & Dataset:**
- Download: [Suicide_Detection.zip](https://drive.google.com/file/d/1qp6-mNpkYyuriUBAGqQ8x7oG6df1xwIn/view?usp=sharing)
- Extract to project root as `Suicide_Detection/`

**Quick Setup:**
```bash
# After downloading both zip files
unzip Offensive_Detection.zip
unzip Suicide_Detection.zip
```

## ğŸ“ Model Training

### Prerequisites

```bash
# Install training dependencies
pip install torch transformers datasets accelerate
```

### 1. Offensive Language Detection Model

**Dataset**: `Offensive_Detection/Dataset/labeled_data.csv` (24,783 samples)
- hate_speech: 1,430 samples (5.8%)
- offensive_language: 19,190 samples (77.4%)
- neither: 4,163 samples (16.8%)

**Training Steps**:

```bash
# Navigate to training scripts directory
cd Training_Scripts

# Open and run the training notebook
jupyter notebook Training_Offensive_Detection.ipynb
```

**Training Notebook**: `Training_Scripts/Training_Offensive_Detection.ipynb`

**Training Configuration**:
- Model: DistilBERT-base-uncased
- Epochs: 3
- Batch Size: 64
- Learning Rate: 2e-5
- Best Checkpoint: checkpoint-840 (after 840 steps)

### 2. Suicide Detection Model

**Dataset**: `Suicide_Detection.csv`
- Binary classification: suicide vs non-suicide
- Mental health conversation data with crisis intervention examples

**Training Steps**:

```bash
# Navigate to training scripts directory
cd Training_Scripts

# Open and run the training notebook
jupyter notebook Training_Suicide_Detection.ipynb
```

**Training Notebook**: `Training_Scripts/Training_Suicide_Detection.ipynb`

**Training Configuration**:
- Model: DistilBERT-base-uncased
- Epochs: 3
- Batch Size: 64
- Learning Rate: 2e-5
- Best Checkpoint: checkpoint-1965 (after 1965 steps)

### Model Evaluation

```bash
# Evaluate trained models
python evaluate_models.py
```

**Performance Metrics**:
- Offensive Detection: 92.1% accuracy, 0.919 F1-score
- Suicide Detection: 97.4% accuracy, 0.974 F1-score

### Run Streamlit Demo

```bash
streamlit run streamlit_demo.py
```

### Run CLI Demo

```bash
python cli_demo.py
```

### Run Basic Pipeline

```bash
python ai_safety_pipeline.py
```

## ğŸ“Š Features

### Real-time Safety Analysis
- Instant message classification
- Multi-model inference pipeline
- Confidence scoring for all predictions

### Escalation Tracking
- User conversation history monitoring
- Pattern recognition for increasing aggression
- Configurable escalation thresholds

### Crisis Intervention
- Suicide/self-harm detection
- Automatic human review triggers
- Emergency response protocols

### Age-Appropriate Filtering
- Rule-based content filtering
- Age-specific restrictions
- Guardian supervision support

## ğŸ® Demo Interface

The Streamlit interface provides:

- **Interactive message testing**
- **Real-time safety analysis**
- **User history tracking**
- **Escalation score monitoring**
- **Sample message testing**

## ğŸ“ˆ Model Performance

### Offensive Language Detection
- Training dataset: 24,783 samples
- Classes distribution:
  - Offensive Language: 77.4%
  - Neither: 16.8%
  - Hate Speech: 5.8%

### Suicide Detection
- Binary classification (suicide/non-suicide)
- Trained on mental health conversation data

## ğŸ”§ Configuration

### Escalation Thresholds
- Default escalation threshold: 60%
- History window: Last 10 messages
- Evaluation window: Last 5 messages

### Age Filtering Rules
- Under 13: Blocks adult keywords
- Under 18: Blocks explicit content
- Configurable keyword lists

## ğŸ›¡ï¸ Safety Actions

| Condition | Action | Human Review |
|-----------|--------|--------------|
| Crisis Detected | BLOCK_AND_ALERT | âœ… Required |
| High Escalation | WARN_AND_MONITOR | âœ… Required |
| Hate Speech | BLOCK | âŒ Optional |
| Offensive Language | WARN | âŒ Optional |
| Age Inappropriate | BLOCK | âŒ Optional |
| Safe Content | ALLOW | âŒ None |

## ğŸ“ Project Structure

```
Solulab/
â”œâ”€â”€ ai_safety_pipeline.py      # Main pipeline implementation
â”œâ”€â”€ streamlit_demo.py          # Web interface demo
â”œâ”€â”€ cli_demo.py               # Command-line demo
â”œâ”€â”€ evaluate_models.py        # Model evaluation script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ .gitattributes           # Git LFS configuration
â”œâ”€â”€ Training_Scripts/        # Model training notebooks
â”‚   â”œâ”€â”€ Training_Offensive_Detection.ipynb
â”‚   â””â”€â”€ Training_Suicide_Detection.ipynb
â”œâ”€â”€ Offensive_Detection/      # Abuse detection model
â”‚   â”œâ”€â”€ Dataset/             # Training dataset
â”‚   â”‚   â””â”€â”€ labeled_data.csv # 24,783 samples
â”‚   â””â”€â”€ Output/              # Trained model checkpoints
â”‚       â””â”€â”€ checkpoint-840/
â””â”€â”€ Suicide_Detection/        # Crisis detection model
    â”œâ”€â”€ Suicide_Detection.csv # Training dataset
    â””â”€â”€ Output/              # Trained model checkpoints
        â””â”€â”€ checkpoint-1965/
```

## ğŸ”® Future Enhancements

### Production Scaling
- API endpoint implementation
- Database integration for user history
- Real-time streaming support
- Load balancing for high throughput

### Model Improvements
- Multi-language support
- Context-aware analysis
- Bias mitigation techniques
- Continuous learning pipeline

### Additional Safety Features
- Image/video content analysis
- Voice message processing
- Behavioral pattern analysis
- Community reporting integration

## ğŸ¤ Leadership Considerations

### Team Development Strategy
1. **Modular Architecture** - Easy to extend with new models
2. **A/B Testing Framework** - Compare model performance
3. **Monitoring Dashboard** - Track safety metrics
4. **Feedback Loop** - Continuous model improvement

### Ethical Guidelines
- Bias detection and mitigation
- Transparency in decision making
- User privacy protection
- Fair treatment across demographics

## ğŸ“ Technical Specifications

- **Framework**: PyTorch + Transformers
- **Models**: DistilBERT-based classifiers
- **Interface**: Streamlit web app
- **Requirements**: CPU-compatible (no GPU needed)
- **Latency**: <100ms per message
- **Scalability**: Horizontal scaling ready

## ğŸ¥ Demo Video

Record a 10-minute walkthrough covering:
1. Architecture explanation
2. Model integration approach
3. Live demo with sample inputs
4. Pros/cons discussion
5. Production scaling considerations

---

**Built for AI Safety - Protecting Users Through Intelligent Content Analysis**